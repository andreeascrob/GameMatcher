{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "762cf4cb-d833-4c95-a912-c40f004e6fb3",
   "metadata": {},
   "source": [
    "# Solution to Exercise 08 - Web Scraping\n",
    "\n",
    "In today's exercise we're using the Python libraries *BeautifulSoup* and *owlready2* to create an ontology from data scraped from the Web.\n",
    "[BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) is a library for extracting data from HTML or XML files by accessing concrete elements in the tree structure.\n",
    "The ontology (& the web scraping) is foused on extracting data about Pok√©mon from a wiki-like website called *[Bulbapedia](https://bulbapedia.bulbagarden.net/wiki/Main_Page)*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92298eed-c4a4-42c3-baa1-22418e404a0e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87421a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "!pip install webdriver-manager\n",
    "!pip install curl_cffi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33732f5-8480-4f5f-a8c2-a9b5b52d124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from curl_cffi import requests \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7980707-c536-4ede-88ab-3b113df126bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TechScraper:\n",
    "    def __init__(self, domain=\"www.cpubenchmark.net\"):\n",
    "        # --- UNIVERSAL PATH SETUP ---\n",
    "        # Get the directory where this script lives\n",
    "        self.script_dir = Path.cwd()\n",
    "        # Target the 'row data' folder (assumes it is in the same parent folder as 'cleaning')\n",
    "        self.save_dir = self.script_dir / \"raw data\"\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.domain = domain\n",
    "        self.is_cpu = \"cpu\" in domain\n",
    "        self.mega_page = f\"https://{domain}/{'CPU' if self.is_cpu else 'GPU'}_mega_page.html\"\n",
    "        \n",
    "        # Absolute path for the JSON file\n",
    "        filename = \"cpu_data.json\" if self.is_cpu else \"gpu_data.json\"\n",
    "        self.cache_file = self.save_dir / filename\n",
    "        \n",
    "        # Load or Scrape logic\n",
    "        self.items = self.load_local()\n",
    "        \n",
    "        if not self.items:\n",
    "            print(f\"--- No local data found. Starting Scrape for {self.domain} ---\")\n",
    "            self.session = requests.Session()\n",
    "            self.items = self.scrape()\n",
    "            if self.items:\n",
    "                self.save_local()\n",
    "\n",
    "    def clean_mark(self, value):\n",
    "        \"\"\"Safely converts benchmark strings to integers, handling 'NA' or 'Insufficient data'.\"\"\"\n",
    "        if value is None:\n",
    "            return 0\n",
    "        val_str = str(value).replace(',', '').strip()\n",
    "        if val_str.replace('-', '', 1).isdigit():\n",
    "            return int(val_str)\n",
    "        return 0\n",
    "\n",
    "    def scrape(self):\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0\",\n",
    "            \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
    "            \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "            \"Referer\": self.mega_page,\n",
    "            \"Connection\": \"keep-alive\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            print(f\"Warming up session for {self.domain}...\")\n",
    "            # Using curl_cffi's impersonate to bypass TLS fingerprinting\n",
    "            self.session.get(self.mega_page, impersonate=\"chrome120\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            print(f\"Requesting hardware list from {self.domain}...\")\n",
    "            data_url = f\"https://{self.domain}/data/?_={int(time.time()*1000)}\"\n",
    "            response = self.session.get(data_url, headers=headers, impersonate=\"chrome120\")\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå Server returned status code {response.status_code}\")\n",
    "                return []\n",
    "\n",
    "            raw_data = response.json().get(\"data\", [])\n",
    "            if not raw_data:\n",
    "                print(\"‚ùå Server returned empty list.\")\n",
    "                return []\n",
    "            \n",
    "            print(f\"‚úÖ Success! Found {len(raw_data)} items.\")\n",
    "            \n",
    "            mark_key = 'cpumark' if self.is_cpu else 'g3dmark'\n",
    "            \n",
    "            return [\n",
    "                {\n",
    "                    \"name\": x.get('name'), \n",
    "                    \"mark\": self.clean_mark(x.get(mark_key)), \n",
    "                    \"rank\": self.clean_mark(x.get('rank'))\n",
    "                } \n",
    "                for x in raw_data\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Scrape failed: {e}\")\n",
    "            return []\n",
    "\n",
    "    def save_local(self):\n",
    "        with open(self.cache_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.items, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"üíæ Data saved to: {self.cache_file.absolute()}\")\n",
    "\n",
    "    def load_local(self):\n",
    "        if self.cache_file.exists():\n",
    "            try:\n",
    "                with open(self.cache_file, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    if data:\n",
    "                        print(f\"üìÇ Loaded {len(data)} items from {self.cache_file.name}.\")\n",
    "                        return data\n",
    "            except (json.JSONDecodeError, IOError):\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "# --- EXAMPLE EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape CPUs\n",
    "    cpu_scraper = TechScraper(domain=\"www.cpubenchmark.net\")\n",
    "    # Scrape GPUs\n",
    "    gpu_scraper = TechScraper(domain=\"www.videocardbenchmark.net\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
